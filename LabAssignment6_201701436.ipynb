{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignment6_201701436.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X_JKE1-qHBpR"
      ],
      "authorship_tag": "ABX9TyM7O0gPnblFMlf3yCkSnn+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarshanPatel0919/Deep-Learning/blob/master/LabAssignment6_201701436.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_JKE1-qHBpR"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hItsTtMKDBjj"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib.pyplot import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y702pVpgGhAY"
      },
      "source": [
        "#Function, Gradient and Global Minima on Contour Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc8mWBboDy_i"
      },
      "source": [
        "def func(x,y):\n",
        "  return (x-y+1.5)**2 + ((2.5+x) - x*y*y)**2 + ((2.625 - x) + x*y*y*y)**2\n",
        "\n",
        "def custom_function(t):\n",
        "  x,y = t\n",
        "  return func(x,y)\n",
        "\n",
        "def gradient(t):\n",
        "  x,y = t\n",
        "  dfdx = 2*((x-y+1.5) + (2.5+x - x*y*y)*(1-y*y) + (2.625 - x + x*y*y*y)*(-1 + y*y*y))\n",
        "  dfdy = 2*((x-y+1.5)*(-1) + (2.5+x - x*y*y)*(-2*x*y) + (2.625 - x + x*y*y*y)*(3*x*y*y))\n",
        "  return np.array([dfdx,dfdy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BnxDwrjyxdJ"
      },
      "source": [
        "#Global Minima and Contour Plot\n",
        "\n",
        "# Below function is used to make sure all the levels of contours look uniform in the plot\n",
        "def normalized_levels(Z, k):\n",
        "    n, m = Z.shape\n",
        "    size = n*m\n",
        "    step = size//k\n",
        "    s = np.sort(Z,1)\n",
        "    level = []\n",
        "    for i in range(0,size,step):\n",
        "      level += [s[i//m,i%m] , s[i//m,-(i%m)-1]]\n",
        "    level = list(dict.fromkeys(sorted(level)))\n",
        "    return level\n",
        "\n",
        "st, en = -2,3\n",
        "r = 500\n",
        "n = int((en-st)*r) + 1\n",
        "x, y = np.linspace(st,en,n), np.linspace(st,en,n)\n",
        "\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = func(X, Y)\n",
        "\n",
        "mn = np.min(Z)\n",
        "idy, idx = np.where(Z==mn)\n",
        "minx, miny = np.around(x[idx],5), np.around(y[idy],5)\n",
        "p = list(zip(minx, miny))\n",
        "p = list(dict.fromkeys(p))[0]\n",
        "\n",
        "print(\"Optimum Solution: \",p)\n",
        "print(\"Minimum Value: \",mn)\n",
        "\n",
        "## Plots the function's contour diagram\n",
        "def plot_function(name = 'Function', xset=[-2,3], yset=[-2,3], r=500, xr=[-2,3], yr=[-2,3]):\n",
        "  ax = gca()\n",
        "  #ax.set_aspect('equal')\n",
        "  \n",
        "  n = int((en-st)*r) + 1\n",
        "  x, y = np.linspace(xset[0],xset[1],1+int((xset[1]-xset[0])*r)), np.linspace(yset[0],yset[1],int((yset[1]-yset[0])*r))\n",
        "\n",
        "  X, Y = np.meshgrid(x, y)\n",
        "  Z = func(X, Y)\n",
        "  \n",
        "  ax.contour(X, Y, Z, normalized_levels(Z, 8),colors=['skyblue'])\n",
        "  grid()\n",
        "  xlabel('x')\n",
        "  ylabel('y')\n",
        "  xlim(xr)\n",
        "  ylim(yr)\n",
        "  title(name)\n",
        "\n",
        "## Plot the path followed by given optimizer\n",
        "\n",
        "## Note. Star: Indicates Final point of path\n",
        "## Note. O :  Indicates Initial point of Path\n",
        "def plot_path(path, color = 'r', name='', st=50, en=100):\n",
        "  plot(path[:,0],path[:,1],color,label=name)\n",
        "  scatter(path[0,0], path[0,1], marker='o', s=st, c=color)\n",
        "  scatter(path[-1,0], path[-1,1], marker='*', s=en, c=color)\n",
        "\n",
        "figure(figsize=(5,5))\n",
        "plot_function()\n",
        "plot_path(np.array([p]))\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeIXIRPb4PDM"
      },
      "source": [
        "# Critical Points\n",
        "print(func(0.502803,-1.56081))\n",
        "print(func(-0.151051,2.12316))\n",
        "print(func(0.29767,-0.83626))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s83DTURFWxr"
      },
      "source": [
        "#Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "743vGeq3Ej2T"
      },
      "source": [
        "## Difference between previous and cuurent state of gradient decent\n",
        "def error(x,y):\n",
        "  return np.sqrt(np.mean((x-y)**2))\n",
        "\n",
        "def print_ans(p, f , n):\n",
        "  print(\"Point: \",np.around(p,4))\n",
        "  print(\"Value: \",np.around(f(p),4))\n",
        "  print(\"Iterations: \",n, '\\n')\n",
        "\n",
        "def SGD(w1, alpha = 0.01, n= 10000, eps = 1e-8):\n",
        "\n",
        "  ## Store each point found during SGD in 'path'\n",
        "  path = []\n",
        "\n",
        "  ## Maximum number of iterations (i.e. EPOCHS) is 'n'\n",
        "  while len(path)<n:\n",
        "\n",
        "      path += [w1]\n",
        "      gd = gradient(w1)\n",
        "      w2 = w1 - alpha*gd\n",
        "\n",
        "      ## If the difference is less than given threshold stop the algorithm\n",
        "      if error(w1,w2) < eps:\n",
        "        break\n",
        "\n",
        "      ## w2 will be the previous weight for the next iteration\n",
        "      w1 = w2\n",
        "\n",
        "  ## Return Optimum Solution along with the path\n",
        "  return (w1,path)\n",
        "\n",
        "def Momentum_NAG(w1, alpha= 0.01, gamma = 0.9, n= 10000, eps = 1e-8):\n",
        "  path = []\n",
        "  d0 = 0\n",
        "  while len(path)<n:\n",
        "    path += [w1]\n",
        "    w_adv = w1 - alpha*d0\n",
        "    gd = gradient(w_adv)\n",
        "    d1 = gamma*d0  + (1-gamma)*gd\n",
        "    w2 = w1 - alpha*d1\n",
        "    if error(w1,w2) < eps:break\n",
        "    w1,d0 = w2,d1\n",
        "  return (w1,path)\n",
        "\n",
        "def RMSProp(w1, alpha=0.01, beta= 0.9, n= 100000, eps= 1e-8):\n",
        "  path = []\n",
        "  h0 = 0\n",
        "  while len(path)<n:\n",
        "      path += [w1]\n",
        "      gd = gradient(w1)\n",
        "      h1 = beta*h0 + (1-beta)*(gd**2)\n",
        "      w2 = w1 - alpha*gd/(np.sqrt(h1)+eps)\n",
        "      if error(w1,w2) < eps:break\n",
        "      w1,h0 = w2,h1\n",
        "  return (w1,path)\n",
        "\n",
        "def adam(w1, alpha=0.1, beta1= 0.9, beta2= 0.99, n= 10000, eps= 1e-8):\n",
        "  path = []\n",
        "  d0,h0 = 0,0\n",
        "  while len(path)<n:\n",
        "      path += [w1]\n",
        "      gd = gradient(w1)\n",
        "      d1 = beta1*d0 + (1-beta1)*gd\n",
        "      h1 = beta2*h0 + (1-beta2)*(gd**2)\n",
        "      w2 = w1 - alpha*d1/(np.sqrt(h1)+eps)\n",
        "      if error(w1,w2) < eps:break\n",
        "      w1,d0,h0 = w2,d1,h1\n",
        "  return (w1,path)\n",
        "  \n",
        "tester = custom_function\n",
        "\n",
        "## Initial Weights\n",
        "w0 = np.array([-2,0])\n",
        "\n",
        "name = ['SGD','Momentum(NAG)','RMSprop','ADAM']\n",
        "optimizer = [SGD, Momentum_NAG, RMSProp, adam]\n",
        "\n",
        "figure(figsize=(20,4.5))\n",
        "PATH = {}\n",
        "for i in range(len(name)):\n",
        "  print(\"========  \" + name[i] + \"  =========\")\n",
        "  subplot(1,4,i+1)\n",
        "  w, path = optimizer[i](np.copy(w0))\n",
        "  PATH[i] = np.copy(path)\n",
        "  print_ans(w, tester, len(path))\n",
        "  plot_function(name[i])\n",
        "  plot_path(np.array(path))\n",
        "show()\n",
        "\n",
        "figure(figsize=(20,5))\n",
        "for i in range(len(name)):\n",
        "  subplot(1,4,i+1)\n",
        "  plot_function(name[i],xset=[-0.6,0],yset=[1.5,2.2],xr=[-0.6,0],yr=[1.5,2.2])\n",
        "  plot_path(np.array(PATH[i]),st=5,en=5)\n",
        "suptitle('Closer Look')\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8xUuLa0FJvV"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "* By Looking at the Contour Plots we observe that the function near minima is very very flat. \n",
        "* SGD shows the smoothest (also slowest) treanstion towards the minima\n",
        "* Momentum Accelarated Gradient decent is Faster compared to SGD always it is relatively smoother and stable than next two optimizers.\n",
        "* Root Mean Square Propogation based optimizer is actually converging faster but as it reaches near the minima frequency of zigzag is so much that it never stays on the minima and keeps zig-zagging around it, hence it keeps on going even if we set iterations to 1 Million it does not converge on one point, but it is assured that it will zig zag very near to minima, so we can apply **normal gradient decent** after fewer iterations of RMS Propagation and get the best out of it.\n",
        "* Adam is also shows zig zag behaviour near the minima and along the path, but it is controlled by both velocity and momentum hence it will converge near the global minima, or in other words we are combining the advatages of Root Mean Square Propagation method and Momentum Accelarated method. Also it overcomes the drawback of SGD since it very faster compared to all of the previous optimizers\n",
        "* Intuitively we might think that learning rate could be increased for other three optimizer and get the better results, but it does not work as well since it results in divergence. It is because ADAM is considering both the cases it does not diverge, hence ADAM is the best optimizer as compared to previous functions.\n",
        "* Note that for very sensitive functions ADAM might show diverging behaviour in that case we need to choose appropriate value of learning rate, but for most of the time ADAM works best for alpha = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWGbHi6yFZ-z"
      },
      "source": [
        "#Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k8tnrERFLVV"
      },
      "source": [
        "colors = ['r','g','b','y']\n",
        "figure(figsize=(5,5))\n",
        "plot_function('Compare',xr=[-2,0.5],yr=[-1,2.5])\n",
        "for i in range(len(name)):\n",
        "  path = PATH[i]\n",
        "  plot_path(np.array(path),colors[i],name[i])\n",
        "legend(bbox_to_anchor=(1.5, 1))\n",
        "show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18DVwOxEI9Rl"
      },
      "source": [
        "* Here we can see that how steady stochastic gradient decent is, but it is very much slower.\n",
        "* Where as ADAM shows unsteady behaviour at first, but it coverges to minima very faster, very helpful for solving problems like vanishing gradient.\n",
        "* Momentum Accelaration Based approach led us to RMS Propagation and ADAM. \n",
        "* Momentum and RMS prop performances are between ADAM and SGD. Using RMS Prop along with SGD might be the trick to improve its performance."
      ]
    }
  ]
}