{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignment2_201701436.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNq0OLVBQEIi2hjbz52bOPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarshanPatel0919/Deep-Learning/blob/master/LabAssignment2_201701436.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUgItTlVhDU"
      },
      "source": [
        "Darshan Patel \n",
        "\n",
        "201701436\n",
        "\n",
        "Deep Learning\n",
        "\n",
        "Lab 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkvY79wAwyFK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pylab as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcOiqJc48WOH"
      },
      "source": [
        "##### Returns an activation function based on index \n",
        "activation_function = [tf.math.sigmoid,tf.nn.softmax,tf.nn.relu,tf.nn.tanh]\n",
        "\n",
        "#### Basic Forward Propogation for given input with given parameters    \n",
        "def NN(input,layers,activation_functions,mean,sigma,start,end):\n",
        "  ### A will be input for next layer\n",
        "  A = input[start:end]\n",
        "  prev_layer_size = tf.cast(tf.shape(input)[1],dtype=tf.int64)\n",
        "\n",
        "  ### Generalized layer array has ith element as number of nodes in ith layer\n",
        "  for layer in range(len(layers)):\n",
        "    current_layer_size = layers[layer]\n",
        "    ### Formulas of forward propogation\n",
        "    W = tf.random.normal([prev_layer_size,current_layer_size],mean,sigma,dtype=tf.dtypes.float64)\n",
        "    B = tf.repeat(tf.random.normal([1,current_layer_size],mean,sigma,dtype=tf.dtypes.float64),repeats= end - start,axis=0)\n",
        "    Z = tf.add(tf.linalg.matmul(A,W),B)\n",
        "    A = tf.Variable(activation_function[layer](Z),dtype=tf.dtypes.float64)\n",
        "    prev_layer_size = current_layer_size\n",
        "  return A\n",
        "\n",
        "### Batch Forward propogation\n",
        "def NN_batch(input,layers,activation_functions,mean,sigma,batches):\n",
        "  all_batch_predictions = []\n",
        "  m = tf.shape(input)[0]\n",
        "  for batch_size in batches:\n",
        "    start = 0\n",
        "    single_batch_prediction = []\n",
        "    while start<m:\n",
        "      pre_data = NN(input,layers,activation_functions,mean,sigma,start,min(start+batch_size,m))\n",
        "      for i in range(tf.shape(pre_data)[0]):\n",
        "        rest_data = []\n",
        "        for j in range(10):\n",
        "          rest_data += [pre_data[i][j]]\n",
        "        single_batch_prediction += [rest_data]\n",
        "      start += batch_size\n",
        "    all_batch_predictions += [single_batch_prediction]\n",
        "  return all_batch_predictions\n",
        "\n",
        "### Different Nodes in given layer\n",
        "def NN_node(input,layerss,activation_functions,mean,sigma):\n",
        "  return [NN(input,layers,activation_functions,mean,sigma,0,tf.shape(input)[0]) for layers in layerss]\n",
        "\n",
        "### Different Activation functions for given layer\n",
        "def NN_activation(input,layers,activation_functionss,mean,sigma):\n",
        "  return [NN(input,layers,activation_functions,mean,sigma,0,tf.shape(input)[0]) for activation_functions in activation_functionss]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDuqmrungKFm"
      },
      "source": [
        "# **Digit Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPRjRO74frPr"
      },
      "source": [
        "### Loading the dataset\n",
        "data = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")[0]\n",
        "\n",
        "### x and y are respectively  images and  labels \n",
        "x , y = data\n",
        "x = tf.reshape(x,[tf.shape(x)[0],tf.shape(x)[1]*tf.shape(x)[2]]) #Input will be #Examples(m) * #Features(n)\n",
        "\n",
        "x = tf.cast(x,dtype=tf.float64)\n",
        "y = tf.cast(y,dtype=tf.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTufdBWj8WbN"
      },
      "source": [
        "### In layers first n-1 elements are for #nodes in hidden layers\n",
        "### Last element of layers will be the #nodes in output layer \n",
        "layers = tf.constant([512,256,128,10],dtype=tf.int64)\n",
        "\n",
        "### activation_functions stores activation function id of respective layer\n",
        "activation_functions = [0,2,1,1] \n",
        "\n",
        "### mean and sigma for generating the random weights\n",
        "mean = tf.constant(0,dtype=tf.dtypes.float64)\n",
        "sigma = tf.constant(1,dtype=tf.dtypes.float64)\n",
        "\n",
        "m = tf.cast(tf.shape(x)[0],dtype=tf.int64)\n",
        "\n",
        "###Sample Output\n",
        "output_forward_prop = NN(x,layers,activation_functions,mean,sigma,0,m)\n",
        "\n",
        "y_d=[]\n",
        "for i in range(tf.shape(x)[0]):\n",
        "  y_d.append(tf.argmax(output_forward_prop[i,:]))\n",
        "\n",
        "conf_mat = tf.math.confusion_matrix(y,y_d)\n",
        "accuracy = tf.cast(tf.linalg.trace(conf_mat),dtype=tf.int64)/m #Accuracy = fraction of results which were predicted correctly\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "tf.print(conf_mat)\n",
        "print(\"Accuracy :\",end=\" \")\n",
        "tf.print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQHMa7w1scBZ"
      },
      "source": [
        "Varying the Batch Sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wVvMB3JnNdx"
      },
      "source": [
        "### batches of different sizes\n",
        "batches=[50,100,500,1000,5000,10000]\n",
        "\n",
        "output_forward_prop_set = NN_batch(x,layers,activation_functions,mean,sigma,batches)\n",
        "\n",
        "accuracy = []\n",
        "for output_forward_prop in output_forward_prop_set:\n",
        "  print(np.shape(output_forward_prop))\n",
        "  y_d=[tf.argmax(output_forward_prop[i]) for i in range(tf.shape(x)[0])]\n",
        "  conf_mat = tf.math.confusion_matrix(y,y_d)\n",
        "  accuracy += [tf.cast(tf.linalg.trace(conf_mat),dtype=tf.int64)/m]\n",
        "\n",
        "print(\"Batches vs Accuracy\")\n",
        "plt.plot(batches,accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ocIXtdURWCo"
      },
      "source": [
        "Observation : Here we can see that changing the size of batches does not affect (changes are random) the accuracy because the data is untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCKjNhzlMjE9"
      },
      "source": [
        "******************************************************************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut-nysbELXts"
      },
      "source": [
        "Varying Number of Nodes in Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlRuYuQJHY9K"
      },
      "source": [
        "### nodes of different numbers\n",
        "layerss=[[512,256,128,10],[5,5,5,10],[128,256,512,10],[2,4,8,10],[55,25,55,10]]\n",
        "\n",
        "output_forward_prop_set = NN_node(x,layerss,activation_functions,mean,sigma)\n",
        "\n",
        "accuracy = []\n",
        "for output_forward_prop in output_forward_prop_set:\n",
        "  y_d=[tf.argmax(output_forward_prop[i,:]) for i in range(tf.shape(x)[0])]\n",
        "  conf_mat = tf.math.confusion_matrix(y,y_d)\n",
        "  accuracy += [tf.cast(tf.linalg.trace(conf_mat),dtype=tf.int64)/m]\n",
        "\n",
        "\n",
        "print(\"Different Nodes sets vs Accuracy\")\n",
        "plt.plot([0,1,2,3,4],accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Uk3bliIro_"
      },
      "source": [
        "Observation : Here we can see that changing the number of nodes does not affect the accuracy because the data is untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SljQr4vyMk25"
      },
      "source": [
        "******************************************************************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUlbKDuMRmOH"
      },
      "source": [
        "Varying Activation Functions in Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf2yiyvTHZNt"
      },
      "source": [
        "activation_functionss=[[0,1,2,3],[0,0,0,0],[2,1,2,0],[1,0,3,2],[0,2,1,1]]\n",
        "\n",
        "output_forward_prop_set = NN_activation(x,layers,activation_functionss,mean,sigma)\n",
        "\n",
        "accuracy = []\n",
        "for output_forward_prop in output_forward_prop_set:\n",
        "  y_d=[tf.argmax(output_forward_prop[i,:]) for i in range(tf.shape(x)[0])]\n",
        "  conf_mat = tf.math.confusion_matrix(y,y_d)\n",
        "  accuracy += [tf.cast(tf.linalg.trace(conf_mat),dtype=tf.int64)/m]\n",
        "\n",
        "print(\"Different Activation function sets vs Accuracy\")\n",
        "plt.plot([0,1,2,3,4],accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjt3I1RJLmeC"
      },
      "source": [
        "Observation : Here we can see that changing the activity functions does not affect the accuracy because the data is untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL8j9xehS13a"
      },
      "source": [
        "#Boston Housing Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDR6qRwCS-c5"
      },
      "source": [
        "### Loading the dataset\n",
        "data = tf.keras.datasets.boston_housing.load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113)[0]\n",
        "\n",
        "### x and y are respectively  images and  labels \n",
        "x , y = data\n",
        "\n",
        "x = tf.cast(x,dtype=tf.float64)\n",
        "y = tf.cast(y,dtype=tf.float64)\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8dPoNvpLumo"
      },
      "source": [
        "Varying Number of Nodes in Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQTWxZ3HTD1U"
      },
      "source": [
        "### nodes of different numbers\n",
        "layerss=[[512,256,128,1],[5,5,5,1],[128,256,512,1],[2,4,8,1],[55,25,55,1]]\n",
        "layers = layerss[0]\n",
        "\n",
        "output_forward_prop_set = NN_node(x,layerss,activation_functions,mean,sigma)\n",
        "\n",
        "MSE = []\n",
        "for y_d in output_forward_prop_set:\n",
        "  MSE += [tf.keras.losses.MSE(y,tf.transpose(y_d))]\n",
        "  \n",
        "print(\"Different Nodes sets vs MSE\")\n",
        "plt.plot([0,1,2,3,4],MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlYK47i_MGQx"
      },
      "source": [
        "Observation : Here we can see that changing the number of nodes does not affect the MSE because the data is untrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGVsOPVMMGQ3"
      },
      "source": [
        "******************************************************************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfY6PCLTMGQ5"
      },
      "source": [
        "Varying Activation Functions in Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KJsy-JmNqeO"
      },
      "source": [
        "activation_functionss=[[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3],[0,2,1,1]]\n",
        "\n",
        "output_forward_prop_set = NN_activation(x,layers,activation_functionss,mean,sigma)\n",
        "\n",
        "MSE = []\n",
        "for y_d in output_forward_prop_set:\n",
        "  MSE += [tf.keras.losses.MSE(y,tf.transpose(y_d))]\n",
        "  \n",
        "print(\"Different Nodes sets vs MSE\")\n",
        "plt.plot([0,1,2,3,4],MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijWQs9ZfMGvy"
      },
      "source": [
        "Observation : Here we can see that changing the activity functions does not actually affect, the effects are random the MSE because the data is untrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I3Me_kRcemh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}